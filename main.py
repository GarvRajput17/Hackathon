pip install transformers
from transformers import BlipProcessor, BlipForConditionalGeneration, DetrImageProcessor, DetrForObjectDetection
from PIL import Image
import torch
import streamlit as st

# Computer Vision Models

def get_image_caption(image_path):
    image = Image.open(image_path).convert('RGB')
    model_name = "Salesforce/blip-image-captioning-large"
    device = "cpu"
    processor = BlipProcessor.from_pretrained(model_name)
    model = BlipForConditionalGeneration.from_pretrained(model_name).to(device)
    inputs = processor(image, return_tensors='pt').to(device)
    output = model.generate(**inputs, max_new_tokens=20)
    caption = processor.decode(output[0], skip_special_tokens=True)
    return caption

def classify_image(image_path):
    from transformers import ViTForImageClassification, ViTImageProcessor
    image = Image.open(image_path).convert('RGB')
    model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')
    processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')
    inputs = processor(image, return_tensors ='pt')
    outputs = model(**inputs)
    logits = outputs.logits
    predicted_class_idx = logits.argmax(-1).item()
    return model.config.id2label[predicted_class_idx]


def detect_objects(image_path):
    image = Image.open(image_path).convert('RGB')
    processor = DetrImageProcessor.from_pretrained("facebook/detr-resnet-50")
    model = DetrForObjectDetection.from_pretrained("facebook/detr-resnet-50", revision="no_timm")
    inputs = processor(images=image, return_tensors="pt")
    outputs = model(**inputs)
    target_sizes = torch.tensor([image.size[::-1]])
    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]
    detections = ""
    for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
        detections += '[{}, {}, {}, {}]'.format(int(box[0]), int(box[1]), int(box[2]), int(box[3]))
        detections += ' {}'.format(model.config.id2label[int(label)])
        detections += ' {}\\n'.format(float(score))
    return detections

#Natural-Language Processing Models

def text_summarize(user_text):
    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
    tokenizer = AutoTokenizer.from_pretrained("google/pegasus-xsum")
    model = AutoModelForSeq2SeqLM.from_pretrained("google/pegasus-xsum")
    inputs = tokenizer(user_text, return_tensors="pt", max_length=1024, truncation=True)
    summary_ids = model.generate(inputs["input_ids"], max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

def QA(user_text, context):
    from transformers import AutoTokenizer, AutoModelForQuestionAnswering
    processor = AutoTokenizer.from_pretrained("deepset/tinyroberta-squad2")
    model = AutoModelForQuestionAnswering.from_pretrained("deepset/tinyroberta-squad2")
    inputs = processor(user_text, context, return_tensors='pt')
    outputs = model(**inputs)

    starting = outputs.start_logits
    ending = outputs.end_logits

    resp_start = torch.argmax(starting)
    resp_end = torch.argmax(ending) + 1

    answer = processor.convert_tokens_to_string(processor.convert_ids_to_tokens(inputs["input_ids"][0][resp_start:resp_end]))
    return answer;

def similarity(sentences):
    from transformers import AutoTokenizer, AutoModel
    import torch.nn.functional as F
    from sklearn.metrics.pairwise import cosine_similarity

    #Mean Pooling:

    def mean_pooling(model_output, attention_mask):
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

    processor = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')
    model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')
    encoded_input = processor(sentences, padding=True, truncation=True, return_tensors='pt')

    with torch.no_grad():
        model_output = model(**encoded_input)

    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])
    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)
    similarity_matrix = cosine_similarity(sentence_embeddings)
    return similarity_matrix

def gen_image(user_text):
    from diffusers import DiffusionPipeline
    pipeline = DiffusionPipeline.from_pretrained("dataautogpt3/OpenDalleV1.1")
    image = pipeline(prompt = user_text).images[0]
    image.save("generated_image.png")
    return "generated_image.png"

#Note, we are using Pipelining here, instead of the original model because it has some CUDA requirements which I was not
#able to work with properly, so in order to use this function/model, it will take almost ~20GB of local space worth of
#model installation and will require 5-10 minutes in order to process the output.


# From Here, we are Editing the front-view of our website

#------


# Set the page config to wide mode
st.set_page_config(layout="wide")

# Set title
st.title('Multi Purpose AI Model')

# Initialize a list to store the conversation
conversation = []

# Set header
st.header('Welcome! How may I assist you today with?')

col1, col2, col3 = st.columns(3)

# Text input
with col1:
    st.header('Computer Vision')
    st.write('Computer Vision is a field of Artificial Intelligence that trains computers to interpret and understand the visual world.')
    st.header('Please upload the image')
    file = st.file_uploader("Upload Here", type=["jpeg", "jpg", "png"])
    if file:
        st.image(file, use_column_width=50)
    with st.expander("See options"):
        option = st.selectbox('Choose an option', ('Options:', 'Generate Image Caption/Describe Image', 'Classify Image', 'Detect objects and their positions in the image'))
        if file:
            if option == 'Generate Image Caption/Describe Image':
                caption = get_image_caption(file)
                conversation.append(f"Bot ðŸ¤–: Sure! Here is a caption for the image you provided: {caption}")
            elif option == 'Classify Image':
                classified_image = classify_image(file)
                conversation.append(f"Bot ðŸ¤–: Sure! Here is the classification of the image you provided: {classified_image}")
            elif option == 'Detect objects and their positions in the image':
                detection = detect_objects(file)
                conversation.append(f"Bot ðŸ¤–: Sure! These are the objects I was able to detect: {detection}")



with col2:
    st.header('Natural Language Processing')
    st.write('Natural Language Processing is a branch of AI that helps computers understand, interpret and manipulate human language.')
    with st.expander("See options"):
        option = st.selectbox('Choose an option', ('Text Summarisation', 'Question & Answer', 'Search for similarity', 'Generate images'))
        if option == 'Text Summarisation':
            st.header('Please enter the text you want me to summarize:')
            user_text = st.text_area('', height = 80)
            if user_text:
                summary = text_summarize(user_text)
                conversation.append(f"Bot ðŸ¤–: Here is the summary for you: \n {summary}")

        elif option == "Question & Answer":
            st.header('Please enter your question')
            user_question = st.text_input('')
            st.header('Please enter the context')
            user_context = st.text_area('', height=50)
            if user_question and user_context:
                answer = QA(user_question, user_context)
                conversation.append(f"Bot ðŸ¤– : Here is your answer:\n {answer}")
            else:
                conversation.append(f"Bot ðŸ¤– : Please fill both question and the context fields properly.")

        elif option == "Search for similarity":
            st.header('Your Original sentence')
            original_sentence = st.text_input('Enter first sentence: ')
            st.header('The first sentence you want to check with')
            first_sentence = st.text_input('Enter second sentence: ')
            st.header('The second sentence you want to check with')
            second_sentence = st.text_input('Enter third sentence: ')
            if original_sentence and first_sentence and second_sentence:
                sentences = [original_sentence, first_sentence, second_sentence]
                similar = similarity(sentences)
                for i in range(len(sentences)):
                    conversation.append(
                        f"Bot ðŸ¤–: Here is your sentence:: {sentences[i]}\n Here is your similarity of the sentence with the original sentence: {similar[i][i]:.2f}")
            else:
                conversation.append(f"Bot ðŸ¤–: Please fill all the fields. Make sure no field is blank")

        elif option == "Generate images":
            st.header('Give your input')
            user_input = st.text_input('')
            if user_input:
                image_gen = gen_image(user_input)
                conversation.append(f"Bot ðŸ¤–: Here is the image generated:")
                st.image(image_gen, use_column_width=100)


# Write agent response


# Display the conversation
st.markdown("## Conversation")
for i in range(len(conversation)):
    if i % 2 == 0:
        st.markdown(f"> {conversation[i]}")
    else:
        st.markdown(f"> ðŸ¤–: {conversation[i]}")

# Add a footer
st.markdown("""
---
Made with â™¥ by Garv Rajput [IMT2023505]
""")
